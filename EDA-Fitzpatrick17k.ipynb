{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d9b4500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Install kagglehub if you haven't:\n",
    "#    pip install kagglehub --upgrade\n",
    "import os, shutil, zipfile, pandas as pd, kagglehub\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "76ab549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è¨  Downloading via kagglehub ‚Ä¶\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.12), please consider upgrading to the latest version (0.3.13).\n",
      "KaggleHub cache folder: C:\\Users\\yrsee\\.cache\\kagglehub\\datasets\\vinitasilaparasetty\\fitzpatrick-classification-by-ethnicity\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "KAGGLE_SLUG = \"vinitasilaparasetty/fitzpatrick-classification-by-ethnicity\"\n",
    "DEST        = Path(\"fitzpatrick17k\")          # final clean layout\n",
    "DEST.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# ‚îÄ‚îÄ 2. Download (kagglehub auto-caches) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"‚è¨  Downloading via kagglehub ‚Ä¶\")\n",
    "dl_path = Path(\n",
    "    kagglehub.dataset_download(KAGGLE_SLUG)   # returns cache path\n",
    ")\n",
    "print(\"KaggleHub cache folder:\", dl_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c0def9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 2. If a ZIP exists, unzip; else use the folder as-is ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "zip_candidates = list(dl_path.glob(\"*.zip\"))\n",
    "if zip_candidates:\n",
    "    zip_file = zip_candidates[0]\n",
    "    print(\"üì¶  Extracting\", zip_file.name)\n",
    "    with zipfile.ZipFile(zip_file) as zf:\n",
    "        zf.extractall(DEST)\n",
    "else:\n",
    "    # The dataset is already extracted ‚Üí copy contents to DEST\n",
    "    for item in dl_path.iterdir():\n",
    "        tgt = DEST / item.name\n",
    "        if tgt.exists():\n",
    "            continue\n",
    "        print(\"üìÅ  Copying\", item.name, \"‚Üí\", tgt)\n",
    "        if item.is_dir():\n",
    "            shutil.copytree(item, tgt)\n",
    "        else:\n",
    "            shutil.copy2(item, tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ebde2a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ  Dataset ready in: C:\\Users\\yrsee\\everything\\ACME-Outreach\\skin-diagnostic-engine\\fitzpatrick17k\n",
      "   ‚îú‚îÄ fitzpatrick17k\\labels.csv\n",
      "   ‚îî‚îÄ fitzpatrick17k\\images\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ 3. Standardize layout: move CSV + images/ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Find the main CSV\n",
    "csv_path = next(DEST.rglob(\"*.csv\"))\n",
    "# Find the images folder (contains jpg / png)\n",
    "img_dir  = next(p for p in DEST.rglob(\"*\") if p.is_dir() and any(p.glob(\"*.jpg\")))\n",
    "\n",
    "if img_dir.resolve() != (DEST / \"images\").resolve():\n",
    "    shutil.move(str(img_dir), DEST / \"images\")\n",
    "\n",
    "print(f\"‚úÖ  Dataset ready in: {DEST.resolve()}\")\n",
    "print(\"   ‚îú‚îÄ\", (DEST / 'labels.csv').relative_to(DEST.parent))\n",
    "print(\"   ‚îî‚îÄ\", (DEST / 'images').relative_to(DEST.parent))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "82a9b829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitzpatrick I‚ÄìVI distribution:\n",
      " phototype\n",
      "I & II    903\n",
      "III       903\n",
      "IV        903\n",
      "V         903\n",
      "VI        903\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ 4. Quick sanity: view distribution ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df = pd.read_csv(DEST / \"labels.csv\")\n",
    "dist = df[\"phototype\"].value_counts().sort_index()\n",
    "print(\"\\nFitzpatrick I‚ÄìVI distribution:\\n\", dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d70d0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# melanin_dataset.py\n",
    "import pandas as pd, torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "FITZ_MAP = {\"I\":0,\"II\":1,\"III\":2,\"IV\":3,\"V\":4,\"VI\":5}\n",
    "\n",
    "def _resolve_path_col(df):\n",
    "    for c in [\"image_path\",\"image\",\"filepath\",\"file\",\"filename\",\"path\"]:\n",
    "        if c in df.columns: return c\n",
    "    raise KeyError(\"No image-path column found. Expected one of: image_path,image,filepath,file,filename,path\")\n",
    "\n",
    "def _to_label(v):\n",
    "    # supports 1‚Äì6 or \"I\"‚Äì\"VI\"\n",
    "    if pd.isna(v): raise ValueError(\"Missing phototype label\")\n",
    "    if isinstance(v, str):\n",
    "        v = v.strip()\n",
    "        if v in FITZ_MAP: return FITZ_MAP[v]\n",
    "        if v.upper() in FITZ_MAP: return FITZ_MAP[v.upper()]\n",
    "        # maybe '3' as string\n",
    "        if v.isdigit(): return int(v)-1\n",
    "    # numeric path\n",
    "    return int(v)-1\n",
    "\n",
    "class MelaninDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_root, transform=None, subset_idx=None,\n",
    "                 label_col=\"phototype\"):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.path_col = _resolve_path_col(self.df)\n",
    "        if label_col not in self.df.columns:\n",
    "            raise KeyError(f\"Label column '{label_col}' not in CSV. Available: {self.df.columns.tolist()}\")\n",
    "        # subset if provided\n",
    "        if subset_idx is not None:\n",
    "            self.df = self.df.iloc[subset_idx].reset_index(drop=True)\n",
    "        self.img_root = Path(img_root)\n",
    "        self.transform = transform\n",
    "        # cache labels\n",
    "        self.labels = self.df[label_col].apply(_to_label).astype(int).tolist()\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        img_path = self.img_root / str(row[self.path_col])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return img, self.labels[i]\n",
    "\n",
    "def make_transforms(img_size=224):\n",
    "    # Geometric & mild spatial augs only (avoid color shifts that distort melanin)\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize(int(img_size*1.15)),\n",
    "        transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                             std=[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize(int(img_size*1.15)),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                             std=[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    return train_tf, val_tf\n",
    "\n",
    "def make_balanced_loader(dataset, batch_size=64, workers=4):\n",
    "    import numpy as np\n",
    "    labels = torch.tensor(dataset.labels, dtype=torch.long)\n",
    "    counts = torch.bincount(labels, minlength=6).float()\n",
    "    weights = 1.0 / torch.clamp(counts, min=1.0)\n",
    "    sample_w = weights[labels]\n",
    "    sampler = WeightedRandomSampler(sample_w.double().numpy().tolist(),\n",
    "                                    num_samples=len(labels), replacement=True)\n",
    "    return DataLoader(dataset, batch_size=batch_size, sampler=sampler,\n",
    "                      num_workers=workers, pin_memory=True, persistent_workers=(workers>0))\n",
    "\n",
    "def make_loader(dataset, batch_size=64, shuffle=False, workers=4):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
    "                      num_workers=workers, pin_memory=True, persistent_workers=(workers>0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "df273d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# If you already have these, you can skip the installs.\n",
    "# Recommended CUDA 12.1 wheels for RTX 40xx:\n",
    "# !pip -q install --upgrade torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip -q install pandas scikit-learn matplotlib Pillow tqdm\n",
    "\n",
    "import sys, torch\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c2f75c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV columns: ['file', 'age', 'gender', 'race', 'phototype']\n",
      "        file    age  gender             race phototype\n",
      "0    100.jpg  20-29  Female       East Asian       III\n",
      "1   1000.jpg  20-29    Male  Latino_Hispanic        IV\n",
      "2  10000.jpg  20-29  Female       East Asian       III\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Point these at your prepared dataset\n",
    "DATA_ROOT = Path(\"fitzpatrick17k\")      # folder you created earlier\n",
    "CSV_PATH  = DATA_ROOT / \"labels.csv\"    # should exist\n",
    "IMG_ROOT  = DATA_ROOT / \"images\"        # should exist\n",
    "\n",
    "assert CSV_PATH.is_file(), f\"Missing CSV at {CSV_PATH}\"\n",
    "assert IMG_ROOT.is_dir(),  f\"Missing images/ at {IMG_ROOT}\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"CSV columns:\", df.columns.tolist())\n",
    "print(df.head(3))\n",
    "\n",
    "# Heuristic to find the image-path column if it isn't named 'image_path'\n",
    "for col in [\"image_path\",\"image\",\"filepath\",\"file\",\"filename\",\"path\"]:\n",
    "    if col in df.columns:\n",
    "        PATH_COL = col\n",
    "        break\n",
    "else:\n",
    "    raise KeyError(\"No image-path column found. Expected one of: image_path,image,filepath,file,filename,path\")\n",
    "\n",
    "LABEL_COL = \"phototype\"  # from your dataset\n",
    "assert LABEL_COL in df.columns, f\"'{LABEL_COL}' not found in CSV\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f5a735fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitzpatrick I‚ÄìVI counts:\n",
      " phototype\n",
      "I & II    903\n",
      "III       903\n",
      "IV        903\n",
      "V         903\n",
      "VI        903\n"
     ]
    }
   ],
   "source": [
    "PLOT = False  # set True if you want a bar chart\n",
    "\n",
    "dist = df[LABEL_COL].value_counts().sort_index()\n",
    "print(\"Fitzpatrick I‚ÄìVI counts:\\n\", dist.to_string())\n",
    "\n",
    "if PLOT:\n",
    "    import matplotlib.pyplot as plt\n",
    "    dist.plot(kind=\"bar\", title=\"Fitzpatrick I‚ÄìVI distribution\")\n",
    "    plt.xlabel(\"Fitzpatrick type\"); plt.ylabel(\"# images\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "adbfea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Map accepts 1‚Äì6 or 'I'‚Äì'VI'\n",
    "FITZ_MAP = {\"I\":0,\"II\":1,\"III\":2,\"IV\":3,\"V\":4,\"VI\":5}\n",
    "def to_label(v):\n",
    "    if isinstance(v, str):\n",
    "        v = v.strip().upper()\n",
    "        if v in FITZ_MAP: return FITZ_MAP[v]\n",
    "        if v.isdigit():   return int(v) - 1\n",
    "    return int(v) - 1\n",
    "\n",
    "class MelaninDataset(Dataset):\n",
    "    def __init__(self, df, img_root: Path, tf=None, path_col=\"image_path\", label_col=\"phototype\"):\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.root = img_root\n",
    "        self.tf = tf\n",
    "        self.path_col = path_col\n",
    "        self.label_col = label_col\n",
    "        self.labels = self.df[label_col].apply(to_label).astype(int).tolist()\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        img = Image.open(self.root / str(row[self.path_col])).convert(\"RGB\")\n",
    "        if self.tf: img = self.tf(img)\n",
    "        return img, self.labels[i]\n",
    "\n",
    "def make_transforms(img_size=224):\n",
    "    # Keep color relatively faithful for melanin; use geometric augs only\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize(int(img_size*1.15)),\n",
    "        transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize(int(img_size*1.15)),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    return train_tf, val_tf\n",
    "\n",
    "def make_balanced_loader(dataset, batch_size=64, workers=4):\n",
    "    labels = torch.tensor(dataset.labels, dtype=torch.long)\n",
    "    counts = torch.bincount(labels, minlength=6).float().clamp(min=1)\n",
    "    weights = 1.0 / counts\n",
    "    sample_w = weights[labels]\n",
    "    sampler = WeightedRandomSampler(sample_w.double().tolist(),\n",
    "                                    num_samples=len(labels),\n",
    "                                    replacement=True)\n",
    "    return DataLoader(dataset, batch_size=batch_size, sampler=sampler,\n",
    "                      num_workers=workers, pin_memory=True,\n",
    "                      persistent_workers=(workers>0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0df7422f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "Python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "OS: Windows-10-10.0.26100-SP0\n"
     ]
    }
   ],
   "source": [
    "# cell 0 \n",
    "# If already installed, you can skip these.\n",
    "# CUDA 12.1 wheels (great for RTX 40xx):\n",
    "# !pip -q install --upgrade torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip -q install pandas matplotlib Pillow tqdm\n",
    "\n",
    "import torch, platform, sys\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"OS:\", platform.platform())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d5d15624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['file', 'age', 'gender', 'race', 'phototype']\n",
      "        file    age  gender             race phototype\n",
      "0    100.jpg  20-29  Female       East Asian       III\n",
      "1   1000.jpg  20-29    Male  Latino_Hispanic        IV\n",
      "2  10000.jpg  20-29  Female       East Asian       III\n"
     ]
    }
   ],
   "source": [
    "# cell 1\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_ROOT = Path(\"fitzpatrick17k\")         # adjust if needed\n",
    "CSV_PATH  = DATA_ROOT / \"labels.csv\"\n",
    "IMG_ROOT  = DATA_ROOT / \"images\"\n",
    "\n",
    "assert CSV_PATH.is_file(), f\"CSV not found: {CSV_PATH}\"\n",
    "assert IMG_ROOT.is_dir(),  f\"images/ not found: {IMG_ROOT}\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(df.head(3))\n",
    "\n",
    "# auto-detect a column holding the image relative path\n",
    "PATH_COL = None\n",
    "for c in [\"image_path\", \"image\", \"filepath\", \"file\", \"filename\", \"path\"]:\n",
    "    if c in df.columns:\n",
    "        PATH_COL = c; break\n",
    "assert PATH_COL is not None, \"Could not find an image path column.\"\n",
    "LABEL_COL = \"phototype\"   # this dataset uses 'phototype'\n",
    "assert LABEL_COL in df.columns, f\"Missing label column '{LABEL_COL}'\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1e58470f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 4515 rows; dropped 0 ambiguous/unparseable.\n",
      "Train: 3840 | Val: 675\n",
      "Train per-class: {1: 768, 2: 768, 3: 768, 4: 768, 5: 768}\n",
      "Val   per-class: {1: 135, 2: 135, 3: 135, 4: 135, 5: 135}\n",
      "Train: 3840 | Val: 675\n",
      "Train per-class: {1: 768, 2: 768, 3: 768, 4: 768, 5: 768}\n",
      "Val   per-class: {1: 135, 2: 135, 3: 135, 4: 135, 5: 135}\n"
     ]
    }
   ],
   "source": [
    "# cell 2\n",
    "import re, numpy as np, pandas as pd\n",
    "\n",
    "VAL_FRAC = 0.15                # 15% validation\n",
    "POLICY   = \"max\"               # how to resolve multi-labels: \"drop\" | \"max\" | \"min\" | \"median\"\n",
    "ROMAN_TO_INT = {\"I\":1,\"II\":2,\"III\":3,\"IV\":4,\"V\":5,\"VI\":6}\n",
    "TOKEN_RE = re.compile(r\"(VI|IV|V|III|II|I|6|5|4|3|2|1)\")  # ‚Üê includes 'V' now\n",
    "\n",
    "def parse_fitz(v, policy=POLICY):\n",
    "    \"\"\"\n",
    "    Return class index 0..5 for I..VI.\n",
    "    - Handles forms like 'I & II', 'III-IV', '3/4', ' v '.\n",
    "    - For multi-tokens uses POLICY to collapse to a single class.\n",
    "    \"\"\"\n",
    "    if pd.isna(v): \n",
    "        return None\n",
    "    s = str(v).upper().strip()\n",
    "    s = s.replace(\"&\",\" \").replace(\"/\", \" \").replace(\"\\\\\",\" \").replace(\"-\", \" \")\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "    toks = TOKEN_RE.findall(s)\n",
    "    if not toks:\n",
    "        return None\n",
    "    vals = []\n",
    "    for t in toks:\n",
    "        if t in ROMAN_TO_INT: vals.append(ROMAN_TO_INT[t])\n",
    "        elif t.isdigit():     vals.append(int(t))\n",
    "    vals = [x for x in vals if 1 <= x <= 6]\n",
    "    if not vals:\n",
    "        return None\n",
    "    if len(vals) == 1:\n",
    "        return vals[0] - 1  # 0-based\n",
    "    # multi-label entry\n",
    "    if policy == \"drop\":   return None\n",
    "    if policy == \"max\":    return max(vals) - 1\n",
    "    if policy == \"min\":    return min(vals) - 1\n",
    "    if policy == \"median\": return int(round(float(np.median(vals)))) - 1\n",
    "    return None\n",
    "\n",
    "# ---- apply parsing\n",
    "df[\"_label_idx\"] = df[LABEL_COL].apply(parse_fitz)\n",
    "n_before = len(df)\n",
    "df_bad   = df[df[\"_label_idx\"].isna()]\n",
    "df_clean = df.dropna(subset=[\"_label_idx\"]).copy()\n",
    "df_clean[\"_label_idx\"] = df_clean[\"_label_idx\"].astype(int)\n",
    "\n",
    "print(f\"Kept {len(df_clean)} rows; dropped {n_before - len(df_clean)} ambiguous/unparseable.\")\n",
    "if len(df_bad):\n",
    "    print(\"Examples of ambiguous/unparsed:\\n\", df_bad[LABEL_COL].value_counts().head(10))\n",
    "\n",
    "# ---- stratified split WITHOUT duplication\n",
    "rng = np.random.RandomState(42)\n",
    "train_idx_all, val_idx_all = [], []\n",
    "\n",
    "for cls, grp in df_clean.groupby(\"_label_idx\", sort=True):\n",
    "    ids = grp.index.to_numpy()\n",
    "    n_val = max(1, int(round(len(ids) * VAL_FRAC)))\n",
    "    val_ids   = rng.choice(ids, size=n_val, replace=False)\n",
    "    train_ids = np.setdiff1d(ids, val_ids, assume_unique=True)\n",
    "    val_idx_all.append(val_ids)\n",
    "    train_idx_all.append(train_ids)\n",
    "\n",
    "train_idx_all = np.concatenate(train_idx_all)\n",
    "val_idx_all   = np.concatenate(val_idx_all)\n",
    "\n",
    "# sanity: no overlap, no duplicates\n",
    "assert len(set(train_idx_all) & set(val_idx_all)) == 0\n",
    "assert len(train_idx_all) + len(val_idx_all) == len(df_clean)\n",
    "\n",
    "df_train = df_clean.loc[train_idx_all].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_val   = df_clean.loc[val_idx_all].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(df_train)} | Val: {len(df_val)}\")\n",
    "print(\"Train per-class:\", df_train[\"_label_idx\"].value_counts().sort_index().to_dict())\n",
    "print(\"Val   per-class:\", df_val[\"_label_idx\"].value_counts().sort_index().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f25039aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "is_windows = os.name == \"nt\"\n",
    "NUM_WORKERS = 0 if is_windows else 4   # Windows Jupyter: avoid spawn issues\n",
    "\n",
    "class MelaninDataset(Dataset):\n",
    "    def __init__(self, df, img_root: Path, tf=None, path_col=\"image_path\", label_col=\"_label_idx\"):\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.root = img_root\n",
    "        self.tf = tf\n",
    "        self.path_col = path_col\n",
    "        self.label_col = label_col\n",
    "        self.labels = self.df[label_col].astype(int).tolist()\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        img = Image.open(self.root / str(row[self.path_col])).convert(\"RGB\")\n",
    "        if self.tf: img = self.tf(img)\n",
    "        return img, self.labels[i]\n",
    "\n",
    "def make_transforms(img_size=224):\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize(int(img_size*1.15)),\n",
    "        transforms.RandomResizedCrop(img_size, scale=(0.8,1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize(int(img_size*1.15)),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    return train_tf, val_tf\n",
    "\n",
    "def make_balanced_loader(dataset, batch_size=64):\n",
    "    labels = torch.tensor(dataset.labels, dtype=torch.long)\n",
    "    counts = torch.bincount(labels, minlength=6).float().clamp(min=1)\n",
    "    weights = 1.0 / counts\n",
    "    sample_w = weights[labels]\n",
    "    sampler = WeightedRandomSampler(sample_w.double().tolist(),\n",
    "                                    num_samples=len(labels), replacement=True)\n",
    "    return DataLoader(dataset, batch_size=batch_size, sampler=sampler,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available(),\n",
    "                      persistent_workers=False)\n",
    "\n",
    "def make_loader(dataset, batch_size=64, shuffle=False):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available(),\n",
    "                      persistent_workers=False)\n",
    "\n",
    "train_tf, val_tf = make_transforms(224)\n",
    "train_ds = MelaninDataset(df_train, IMG_ROOT, train_tf, PATH_COL, \"_label_idx\")\n",
    "val_ds   = MelaninDataset(df_val,   IMG_ROOT, val_tf,   PATH_COL, \"_label_idx\")\n",
    "\n",
    "train_loader = make_balanced_loader(train_ds, batch_size=64)\n",
    "val_loader   = make_loader(val_ds,   batch_size=64, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cc15c448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yrsee\\AppData\\Local\\Temp\\ipykernel_22408\\4261742581.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(model.fc.in_features, 6)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "torch.backends.cudnn.benchmark = True  # speedup on CUDA\n",
    "# Optional: PyTorch 2.x compile for extra speed (comment out if issues)\n",
    "# model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "df5d37b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved: 4515  |  Missing: 0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import unicodedata\n",
    "import difflib\n",
    "\n",
    "DATA_ROOT = Path(\"fitzpatrick17k\")\n",
    "CSV_PATH  = DATA_ROOT / \"labels.csv\"\n",
    "IMG_ROOT  = DATA_ROOT / \"images\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Detect the image-path column\n",
    "for c in [\"image_path\",\"image\",\"filepath\",\"file\",\"filename\",\"path\"]:\n",
    "    if c in df.columns:\n",
    "        PATH_COL = c; break\n",
    "else:\n",
    "    raise KeyError(\"No image-path column found.\")\n",
    "\n",
    "# --- index all actual files under images/ (case-insensitive) ---\n",
    "name_map = {}   # basename -> relative Path\n",
    "stem_map = {}   # stem -> [relative Path,...]\n",
    "rel_map = {}    # full relative path (str) -> relative Path\n",
    "\n",
    "for p in IMG_ROOT.rglob(\"*\"):\n",
    "    if not p.is_file(): \n",
    "        continue\n",
    "    rel = p.relative_to(IMG_ROOT)\n",
    "    # normalize unicode and lowercase for robust matching\n",
    "    name = unicodedata.normalize(\"NFKC\", p.name).lower()\n",
    "    stem = unicodedata.normalize(\"NFKC\", p.stem).lower()\n",
    "    rel_s = str(rel).replace(\"\\\\\", \"/\").lower()\n",
    "    name_map[name] = rel\n",
    "    stem_map.setdefault(stem, []).append(rel)\n",
    "    rel_map[rel_s] = rel\n",
    "\n",
    "def _clean_input(raw):\n",
    "    \"\"\"Normalize CSV input: handle NaN, strip quotes, unquote URLs, normalize unicode.\"\"\"\n",
    "    if pd.isna(raw):\n",
    "        return None\n",
    "    s = str(raw).strip()\n",
    "    # drop surrounding quotes if present\n",
    "    if (s.startswith('\"') and s.endswith('\"')) or (s.startswith(\"'\") and s.endswith(\"'\")):\n",
    "        s = s[1:-1].strip()\n",
    "    # if it's a URL, parse path\n",
    "    parsed = urllib.parse.urlparse(s)\n",
    "    path = parsed.path if parsed.scheme else s\n",
    "    # remove query/fragments, percent-decode, normalize unicode\n",
    "    path = urllib.parse.unquote(path.split(\"?\")[0].split(\"#\")[0])\n",
    "    path = unicodedata.normalize(\"NFKC\", path).replace(\"\\\\\", \"/\")\n",
    "    return path\n",
    "\n",
    "def resolve_path(raw):\n",
    "    \"\"\"Return a relative path from IMG_ROOT (string) or None if not found.\"\"\"\n",
    "    path = _clean_input(raw)\n",
    "    if not path:\n",
    "        return None\n",
    "\n",
    "    base = Path(path).name.lower()\n",
    "    base = unicodedata.normalize(\"NFKC\", base)\n",
    "\n",
    "    # 1) exact basename match (case-insensitive)\n",
    "    if base in name_map:\n",
    "        return str(name_map[base])\n",
    "\n",
    "    # 2) match by stem (handles .jpg / .jpeg / .png differences)\n",
    "    stem = Path(base).stem.lower()\n",
    "    if stem in stem_map and len(stem_map[stem]) > 0:\n",
    "        return str(stem_map[stem][0])\n",
    "\n",
    "    # 3) exact relative-path match if CSV contains subpaths like \"subdir/xxx.jpg\"\n",
    "    rel_try = path.lower().lstrip(\"./\")\n",
    "    if rel_try in rel_map:\n",
    "        return str(rel_map[rel_try])\n",
    "\n",
    "    # 4) endswith match: CSV might include partial relative path or extra prefix\n",
    "    #    find any indexed relative path that ends with the CSV-provided tail\n",
    "    tail = rel_try.split(\"/\")[-1]\n",
    "    for k, v in rel_map.items():\n",
    "        if k.endswith(\"/\" + tail) or k == tail:\n",
    "            return str(v)\n",
    "\n",
    "    # 5) try unquoted/decoded base again (already done in _clean_input) - still fallback to fuzzy match\n",
    "    # fuzzy match on basenames with a high cutoff\n",
    "    candidates = difflib.get_close_matches(base, name_map.keys(), n=1, cutoff=0.85)\n",
    "    if candidates:\n",
    "        return str(name_map[candidates[0]])\n",
    "\n",
    "    return None\n",
    "\n",
    "df[\"resolved_path\"] = df[PATH_COL].apply(resolve_path)\n",
    "\n",
    "missing = df[\"resolved_path\"].isna().sum()\n",
    "print(f\"Resolved: {len(df) - missing}  |  Missing: {missing}\")\n",
    "\n",
    "if missing:\n",
    "    print(\"Example unresolved entries (original CSV value -> suggestion):\")\n",
    "    unresolved = df.loc[df[\"resolved_path\"].isna(), [PATH_COL]].head(20)\n",
    "    for raw in unresolved[PATH_COL].tolist():\n",
    "        cleaned = _clean_input(raw)\n",
    "        base = Path(cleaned).name if cleaned else \"<<<empty/nan>>>\"\n",
    "        # give top 3 fuzzy candidates to aid debugging\n",
    "        fuzzy = difflib.get_close_matches(str(base).lower(), list(name_map.keys()), n=3, cutoff=0.6)\n",
    "        print(f\"  {raw!r}  -> basename: {base!r}  fuzzy: {fuzzy}\")\n",
    "\n",
    "# Drop truly missing files\n",
    "df = df.dropna(subset=[\"resolved_path\"]).reset_index(drop=True)\n",
    "PATH_COL = \"resolved_path\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "775f1935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows kept: 4515  |  Dropped (ambiguous/unparseable): 0\n",
      "Train: 3840 | Val: 675\n",
      "Train per-class: {1: 768, 2: 768, 3: 768, 4: 768, 5: 768}\n",
      "Val   per-class: {1: 135, 2: 135, 3: 135, 4: 135, 5: 135}\n"
     ]
    }
   ],
   "source": [
    "# === FIXED Cell: robust parsing + correct stratified split (no sklearn) ===\n",
    "import re, numpy as np, pandas as pd\n",
    "\n",
    "LABEL_COL = \"phototype\"     # change if needed\n",
    "VAL_FRAC  = 0.15            # 15% validation\n",
    "POLICY    = \"max\"           # \"drop\" | \"max\" | \"min\" | \"median\"\n",
    "\n",
    "ROMAN_TO_INT = {\"I\":1,\"II\":2,\"III\":3,\"IV\":4,\"V\":5,\"VI\":6}\n",
    "TOKEN_RE = re.compile(r\"(VI|IV|V|III|II|I|6|5|4|3|2|1)\")  # ‚Üê includes V\n",
    "\n",
    "def parse_fitz(v, policy=POLICY):\n",
    "    \"\"\"Return 0..5 for I..VI; handle 'I & II', 'III-IV', '3/4', etc.\"\"\"\n",
    "    if pd.isna(v): \n",
    "        return None\n",
    "    s = str(v).upper().strip()\n",
    "    s = s.replace(\"&\",\" \").replace(\"/\", \" \").replace(\"\\\\\",\" \").replace(\"-\", \" \")\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "    toks = TOKEN_RE.findall(s)\n",
    "    if not toks: \n",
    "        return None\n",
    "    vals = []\n",
    "    for t in toks:\n",
    "        if t in ROMAN_TO_INT: vals.append(ROMAN_TO_INT[t])\n",
    "        elif t.isdigit():     vals.append(int(t))\n",
    "    vals = [x for x in vals if 1 <= x <= 6]\n",
    "    if not vals: return None\n",
    "    if len(vals) == 1: return vals[0] - 1  # 0-based\n",
    "    # multi-token entry\n",
    "    if policy == \"drop\":   return None\n",
    "    if policy == \"max\":    return max(vals) - 1\n",
    "    if policy == \"min\":    return min(vals) - 1\n",
    "    if policy == \"median\": return int(round(float(np.median(vals)))) - 1\n",
    "    return None\n",
    "\n",
    "# Parse labels\n",
    "n_before = len(df)\n",
    "df[\"_label_idx\"] = df[LABEL_COL].apply(parse_fitz)\n",
    "dropped = df[\"_label_idx\"].isna().sum()\n",
    "df_clean = df.dropna(subset=[\"_label_idx\"]).copy()\n",
    "df_clean[\"_label_idx\"] = df_clean[\"_label_idx\"].astype(int)\n",
    "\n",
    "print(f\"Rows kept: {len(df_clean)}  |  Dropped (ambiguous/unparseable): {dropped}\")\n",
    "if dropped:\n",
    "    print(\"Most common unparsed labels:\")\n",
    "    print(df.loc[df[\"_label_idx\"].isna(), LABEL_COL].value_counts().head(10))\n",
    "\n",
    "# Correct stratified split (no duplicates, no overlap)\n",
    "rng = np.random.RandomState(42)\n",
    "train_idx_all, val_idx_all = [], []\n",
    "\n",
    "for cls, grp in df_clean.groupby(\"_label_idx\", sort=True):\n",
    "    ids = grp.index.to_numpy()\n",
    "    n_val = max(1, int(round(len(ids) * VAL_FRAC)))\n",
    "    val_ids   = rng.choice(ids, size=n_val, replace=False)\n",
    "    train_ids = np.setdiff1d(ids, val_ids, assume_unique=True)\n",
    "    val_idx_all.append(val_ids)\n",
    "    train_idx_all.append(train_ids)\n",
    "\n",
    "train_idx_all = np.concatenate(train_idx_all)\n",
    "val_idx_all   = np.concatenate(val_idx_all)\n",
    "\n",
    "# Sanity checks\n",
    "assert len(set(train_idx_all) & set(val_idx_all)) == 0\n",
    "assert len(train_idx_all) + len(val_idx_all) == len(df_clean)\n",
    "\n",
    "df_train = df_clean.loc[train_idx_all].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_val   = df_clean.loc[val_idx_all].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(df_train)} | Val: {len(df_val)}\")\n",
    "print(\"Train per-class:\", df_train[\"_label_idx\"].value_counts().sort_index().to_dict())\n",
    "print(\"Val   per-class:\", df_val[\"_label_idx\"].value_counts().sort_index().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bf838a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "is_windows = os.name == \"nt\"\n",
    "NUM_WORKERS = 0 if is_windows else 4  # Jupyter on Windows: safer with 0\n",
    "\n",
    "class MelaninDataset(Dataset):\n",
    "    def __init__(self, df, img_root: Path, tf=None, path_col=\"resolved_path\", label_col=\"_label_idx\"):\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.root = img_root\n",
    "        self.tf = tf\n",
    "        self.path_col = path_col\n",
    "        self.label_col = label_col\n",
    "        self.labels = self.df[label_col].astype(int).tolist()\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        # Robust open with fallback: if file disappeared, pick another row\n",
    "        tries, idx = 0, i\n",
    "        while tries < 3:\n",
    "            row = self.df.iloc[idx]\n",
    "            p = self.root / str(row[self.path_col])\n",
    "            try:\n",
    "                img = Image.open(p).convert(\"RGB\")\n",
    "                if self.tf: img = self.tf(img)\n",
    "                return img, self.labels[idx]\n",
    "            except FileNotFoundError:\n",
    "                tries += 1\n",
    "                idx = (idx + 1) % len(self.df)\n",
    "        raise FileNotFoundError(f\"Could not load image after retries starting at index {i}\")\n",
    "\n",
    "def make_transforms(img_size=224):\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize(int(img_size*1.15)),\n",
    "        transforms.RandomResizedCrop(img_size, scale=(0.8,1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize(int(img_size*1.15)),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    return train_tf, val_tf\n",
    "\n",
    "def make_balanced_loader(dataset, batch_size=64):\n",
    "    labels = torch.tensor(dataset.labels, dtype=torch.long)\n",
    "    counts = torch.bincount(labels, minlength=6).float().clamp(min=1)\n",
    "    weights = 1.0 / counts\n",
    "    sample_w = weights[labels]\n",
    "    sampler = WeightedRandomSampler(sample_w.double().tolist(),\n",
    "                                    num_samples=len(labels), replacement=True)\n",
    "    return DataLoader(dataset, batch_size=batch_size, sampler=sampler,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available(),\n",
    "                      persistent_workers=False)\n",
    "\n",
    "def make_loader(dataset, batch_size=64, shuffle=False):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available(),\n",
    "                      persistent_workers=False)\n",
    "\n",
    "train_tf, val_tf = make_transforms(224)\n",
    "train_ds = MelaninDataset(df_train, IMG_ROOT, train_tf)\n",
    "val_ds   = MelaninDataset(df_val,   IMG_ROOT, val_tf)\n",
    "\n",
    "train_loader = make_balanced_loader(train_ds, batch_size=64)\n",
    "val_loader   = make_loader(val_ds,   batch_size=64, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cb262074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yrsee\\AppData\\Local\\Temp\\ipykernel_22408\\599003956.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  imgs, labels = imgs.to(device, non_blocking=True), torch.tensor(labels).to(device)\n",
      "C:\\Users\\yrsee\\AppData\\Local\\Temp\\ipykernel_22408\\599003956.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss 1.7186 | val_bal_acc 0.1753 | 55.7s\n",
      "  ‚úÖ saved best checkpoint\n",
      "Epoch 02 | train_loss 1.5155 | val_bal_acc 0.1765 | 37.1s\n",
      "  ‚úÖ saved best checkpoint\n",
      "Epoch 03 | train_loss 1.4145 | val_bal_acc 0.1679 | 37.5s\n",
      "Epoch 04 | train_loss 1.2924 | val_bal_acc 0.1741 | 40.7s\n",
      "Epoch 05 | train_loss 1.1654 | val_bal_acc 0.1593 | 32.6s\n",
      "Epoch 06 | train_loss 1.0446 | val_bal_acc 0.1840 | 36.8s\n",
      "  ‚úÖ saved best checkpoint\n",
      "Epoch 07 | train_loss 0.9245 | val_bal_acc 0.1840 | 40.9s\n",
      "Epoch 08 | train_loss 0.8077 | val_bal_acc 0.1642 | 37.0s\n",
      "Epoch 09 | train_loss 0.7355 | val_bal_acc 0.1790 | 42.9s\n",
      "Epoch 10 | train_loss 0.6567 | val_bal_acc 0.1914 | 33.6s\n",
      "  ‚úÖ saved best checkpoint\n",
      "Epoch 11 | train_loss 0.6284 | val_bal_acc 0.1864 | 37.4s\n",
      "Epoch 12 | train_loss 0.5754 | val_bal_acc 0.1790 | 31.5s\n",
      "Best balanced accuracy: 0.1914\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, time\n",
    "from pathlib import Path\n",
    "\n",
    "def confusion_matrix_np(y_true, y_pred, num_classes=6):\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[t, p] += 1\n",
    "    return cm\n",
    "\n",
    "def balanced_accuracy_from_cm(cm):\n",
    "    per_class = cm.diagonal() / np.clip(cm.sum(axis=1), 1, None)\n",
    "    return float(per_class.mean())\n",
    "\n",
    "EPOCHS = 12\n",
    "save_dir = Path(\"runs/melanin_nb\"); save_dir.mkdir(parents=True, exist_ok=True)\n",
    "best_bacc = -1.0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    # --- train ---\n",
    "    model.train()\n",
    "    running_loss, n = 0.0, 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device, non_blocking=True), torch.tensor(labels).to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        n += imgs.size(0)\n",
    "\n",
    "    # --- validate ---\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            logits = model(imgs)\n",
    "            pred = logits.argmax(1).cpu().numpy()\n",
    "            y_pred.extend(pred.tolist())\n",
    "            y_true.extend(labels)\n",
    "\n",
    "    cm_val = confusion_matrix_np(y_true, y_pred, num_classes=6)\n",
    "    bacc = balanced_accuracy_from_cm(cm_val)\n",
    "    print(f\"Epoch {epoch:02d} | train_loss {running_loss/n:.4f} | val_bal_acc {bacc:.4f} | {(time.time()-t0):.1f}s\")\n",
    "\n",
    "    if bacc > best_bacc:\n",
    "        best_bacc = bacc\n",
    "        torch.save({\"state_dict\": model.state_dict(),\n",
    "                    \"arch\": \"resnet18\", \"img_size\": 224},\n",
    "                   save_dir / \"best_model.pth\")\n",
    "        np.save(save_dir / \"confmat_val.npy\", cm_val)\n",
    "        print(\"  ‚úÖ saved best checkpoint\")\n",
    "\n",
    "print(\"Best balanced accuracy:\", f\"{best_bacc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "af264709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: recall=0.0000\n",
      "II: recall=0.2148\n",
      "III: recall=0.2667\n",
      "IV: recall=0.1778\n",
      "V: recall=0.2593\n",
      "VI: recall=0.2296\n",
      "\n",
      "Balanced accuracy: 0.1914\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cm = np.load(save_dir / \"confmat_val.npy\")\n",
    "classes = [\"I\",\"II\",\"III\",\"IV\",\"V\",\"VI\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "im = ax.imshow(cm, cmap=\"Blues\")\n",
    "ax.set_xticks(range(6)); ax.set_yticks(range(6))\n",
    "ax.set_xticklabels(classes); ax.set_yticklabels(classes)\n",
    "ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\"); ax.set_title(\"Confusion Matrix\")\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "per_class_recall = cm.diagonal() / np.clip(cm.sum(axis=1), 1, None)\n",
    "for c, r in zip(classes, per_class_recall):\n",
    "    print(f\"{c}: recall={r:.4f}\")\n",
    "print(f\"\\nBalanced accuracy: {per_class_recall.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e6b149f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 4515\n",
      "Parsed label counts (0..5):\n",
      "_label_idx\n",
      "1    903\n",
      "2    903\n",
      "3    903\n",
      "4    903\n",
      "5    903\n",
      "Name: count, dtype: int64\n",
      "Example paths: ['100.jpg', '1000.jpg', '1000.jpg', '1001.jpg', '10004.jpg']\n",
      "Random baseline: 0.167; Majority-class baseline: 0.200\n"
     ]
    }
   ],
   "source": [
    "# 1A. class distribution and number of examples\n",
    "print(\"Total rows:\", len(df))\n",
    "print(\"Parsed label counts (0..5):\")\n",
    "print(df[\"_label_idx\"].value_counts().sort_index())\n",
    "\n",
    "# 1B. how many resolved image files exist (if you resolved paths earlier)\n",
    "# replace PATH_COL with your final path column name (e.g., \"resolved_path\" or original)\n",
    "print(\"Example paths:\", df[PATH_COL].head(5).tolist())\n",
    "\n",
    "# 1C. baseline numbers\n",
    "num_classes = 6\n",
    "random_baseline = 1.0/num_classes\n",
    "majority_class = df[\"_label_idx\"].value_counts().max() / len(df)\n",
    "print(f\"Random baseline: {random_baseline:.3f}; Majority-class baseline: {majority_class:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cdb8a086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64 1 5\n",
      "Linear(in_features=512, out_features=6, bias=True)\n",
      "imgs torch.Size([64, 3, 224, 224]) labels torch.Size([64]) labels min/max 1 5\n"
     ]
    }
   ],
   "source": [
    "# check label dtype and range\n",
    "print(df[\"_label_idx\"].dtype, df[\"_label_idx\"].min(), df[\"_label_idx\"].max())\n",
    "\n",
    "# check model final layer dimension\n",
    "print(model.fc)   # ensure out_features == 6\n",
    "\n",
    "# check one batch shapes from your DataLoader\n",
    "for imgs, labels in train_loader:\n",
    "    print(\"imgs\", imgs.shape, \"labels\", labels.shape, \"labels min/max\", labels.min().item(), labels.max().item())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0073fa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yrsee\\AppData\\Local\\Temp\\ipykernel_22408\\1581629786.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(save_dir / \"best_model.pth\", map_location=device)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "CLASSES = [\"I\",\"II\",\"III\",\"IV\",\"V\",\"VI\"]\n",
    "\n",
    "ckpt = torch.load(save_dir / \"best_model.pth\", map_location=device)\n",
    "model_inf = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "model_inf.fc = nn.Linear(model_inf.fc.in_features, 6)\n",
    "model_inf.load_state_dict(ckpt[\"state_dict\"])\n",
    "model_inf.eval().to(device)\n",
    "\n",
    "infer_tf = transforms.Compose([\n",
    "    transforms.Resize(int(224*1.15)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_image(rel_path: str):\n",
    "    img = Image.open(IMG_ROOT / rel_path).convert(\"RGB\")\n",
    "    x = infer_tf(img).unsqueeze(0).to(device, non_blocking=True)\n",
    "    probs = torch.softmax(model_inf(x), dim=1)[0].cpu().numpy()\n",
    "    idx = int(np.argmax(probs))\n",
    "    return {\"class\": CLASSES[idx], \"index\": idx, \"probs\": probs.tolist()}\n",
    "\n",
    "# Example:\n",
    "# predict_image(df_val.iloc[0][PATH_COL])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
